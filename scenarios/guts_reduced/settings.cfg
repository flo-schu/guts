[case-study]
output = .
data = ./data
observations = tox.db
simulation = SingleSubstanceSim2

[simulation]
# model specification
# --------------------
model = tktd_guts_reduced
modeltype = deterministic
solver = simplified_ode_solver
solver_post_processing = calculate_psurv
y0 = cext=cext D=Array([0])
seed = 1

# data description
# --------------------
dimensions = id time
evaluator_dim_order = id time substance
prior_dimensions = substance
substance = diuron diclofenac naproxen
data_variables = cext survival
data_variables_max = nan 1
data_variables_min = 0 0

# data selection
# --------------------
apical_effect = lethal
hpf = 24

# experiment selection
# --------------------
# the IDs may change when the database is rewritten. Therefore it is potentially
# unsafe to exclude experiments by ID. Better would be a (name, date) pair. Or
# even better would be not to overwrite the database each time I change the layout,
# but to modify the database.
# eid=2 is the Diuron experiment by Kodritsch. This has quite a different relation
# between Ci and Ce. Assuming cext_nom here would probably be better.
# Kodritsch data has IDs: 1,2,3,7,10
# Knapp Data from Diclofenac are also extremely differen
exclude_experiments = 15 16 18 31 42 45 46 2 37 38 39
exclude_treatments = 205


[free-model-parameters]
# IN THIS SCNEARIO PRIORS ARE DEFINED IN THE USER_DEFINED_PROBABILITY_MODEL
# prob.model_rna_pulse_3_nb_independent

# dominant rate constant for scaled damage
k_d.value = 0.01 0.01 0.001
k_d.prior = lognorm(scale=[0.01 0.01 0.01],s=2)

h_b.value = 0.0000001 0.0000001 0.0000001
h_b.prior = lognorm(scale=[0.00000001 0.0000001 0.0000001],s=2)

z.value = 1 1 1
z.prior = lognorm(scale=[1 1 1],s=2)

kk.value = 0.02 0.02 0.02
kk.prior = lognorm(scale=[0.02 0.02 0.02],s=2)

[fixed-model-parameters]
volume_ratio.value = inf

[error-model]
survival = binom(p=survival,n=nzfe)

[multiprocessing]
cores = 1

[inference]
backend = numpyro
extra_vars = nzfe substance_index survivors_before_t
EPS=1e-8
objective_function = objective_average
n_objectives = 1
n_predictions = 1000
plot_function = pyabc_posterior_predictions

[inference.pyabc]
population_size = 100
minimum_epsilon = 0.00001
min_eps_diff = 0.0000001
max_nr_populations = 100
sampler = SingleCoreSampler
database_path = pyabc.db
eval.model_id = 0
eval.history_id = -1
eval.n_predictions = 1000
plot_function = pyabc_posterior_predictions

[inference.pyabc.redis]
port = 1803
password = simulate


[inference.pymoo]
population_size = 1000
max_nr_populations = 100
algorithm = UNSGA3
ftol = 0.01
xtol = 0.001
verbose = True

[inference.numpyro]
user_defined_probability_model = model_guts_reduced
gaussian_base_distribution = 1
kernel = nuts
# With 'svi' or 'map' the model should run with roundabout 20-30 iterations per second. 
# In the init phase this can be around 10 iterations. Slower model evaluations indicate
# that the model is not converging correctly and is perhaps stuck on a local optimum
svi_iterations = 5000
svi_learning_rate = 0.01
chains = 1
draws = 2000
